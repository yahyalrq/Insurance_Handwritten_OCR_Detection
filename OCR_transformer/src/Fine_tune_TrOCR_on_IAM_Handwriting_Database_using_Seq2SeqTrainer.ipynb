{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.0.0+cu118\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(torch.__version__)\n",
        "\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pkSzlRJq68tH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'%pip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!%pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a8eZ6PWTHriw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'%pip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!%pip install -q datasets jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'%pip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!%pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsTaPrDR7My2"
      },
      "source": [
        "## Prepare data\n",
        "\n",
        "We first download the data. Here, I'm just using the IAM test set, as this was released by the TrOCR authors in the unilm repository. It can be downloaded from [this page](https://github.com/microsoft/unilm/tree/master/trocr). \n",
        "\n",
        "Let's make a [regular PyTorch dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html). We first create a Pandas dataframe with 2 columns. Each row consists of the file name of an image, and the corresponding text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "KkHqJw-W9Abl",
        "outputId": "0db0a26a-4749-4b28-cd6f-fb355ad6aaf5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(r\"../df.csv\")\n",
        "df.dropna(inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>file_name</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Fuente0parte_amistoso_6_10.jpg_0.jpg</td>\n",
              "      <td>28/7/2014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Fuente0parte_amistoso_6_10.jpg_1.jpg</td>\n",
              "      <td>14:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Fuente0parte_amistoso_6_10.jpg_10.jpg</td>\n",
              "      <td>C/ ARIAS 56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>Fuente0parte_amistoso_6_10.jpg_11.jpg</td>\n",
              "      <td>22211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12</td>\n",
              "      <td>Fuente0parte_amistoso_6_10.jpg_12.jpg</td>\n",
              "      <td>ESPANA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156250</th>\n",
              "      <td>24123</td>\n",
              "      <td>Fuente10parte_amistoso_9_39.jpg_91.jpg</td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156251</th>\n",
              "      <td>134735</td>\n",
              "      <td>Fuente5parte_amistoso_8_6.jpg_91.jpg</td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156252</th>\n",
              "      <td>56388</td>\n",
              "      <td>Fuente2parte_amistoso_5_37.jpg_4.jpg</td>\n",
              "      <td>x</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156253</th>\n",
              "      <td>143406</td>\n",
              "      <td>Fuente6parte_amistoso_8_21.jpg_104.jpg</td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156254</th>\n",
              "      <td>96283</td>\n",
              "      <td>Fuente4parte_amistoso_4_18.jpg_92.jpg</td>\n",
              "      <td>x</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>156255 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Unnamed: 0                               file_name         text\n",
              "0                0    Fuente0parte_amistoso_6_10.jpg_0.jpg    28/7/2014\n",
              "1                1    Fuente0parte_amistoso_6_10.jpg_1.jpg        14:36\n",
              "2                2   Fuente0parte_amistoso_6_10.jpg_10.jpg  C/ ARIAS 56\n",
              "3                8   Fuente0parte_amistoso_6_10.jpg_11.jpg        22211\n",
              "4               12   Fuente0parte_amistoso_6_10.jpg_12.jpg       ESPANA\n",
              "...            ...                                     ...          ...\n",
              "156250       24123  Fuente10parte_amistoso_9_39.jpg_91.jpg            X\n",
              "156251      134735    Fuente5parte_amistoso_8_6.jpg_91.jpg            X\n",
              "156252       56388    Fuente2parte_amistoso_5_37.jpg_4.jpg            x\n",
              "156253      143406  Fuente6parte_amistoso_8_21.jpg_104.jpg            X\n",
              "156254       96283   Fuente4parte_amistoso_4_18.jpg_92.jpg            x\n",
              "\n",
              "[156255 rows x 3 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJlVYVal9Ojy"
      },
      "source": [
        "We split up the data into training + testing, using sklearn's `train_test_split` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6qLVT1TPN8Nt"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.25, random_state=42) \n",
        "\n",
        "\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "val_df.reset_index(drop=True, inplace=True)\n",
        "test_df.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwlEBh6B9RTE"
      },
      "source": [
        "Each element of the dataset should return 2 things:\n",
        "* `pixel_values`, which serve as input to the model.\n",
        "* `labels`, which are the `input_ids` of the corresponding text in the image.\n",
        "\n",
        "We use `TrOCRProcessor` to prepare the data for the model. `TrOCRProcessor` is actually just a wrapper around a `ViTFeatureExtractor` (which can be used to resize + normalize images) and a `RobertaTokenizer` (which can be used to encode and decode text into/from `input_ids`). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qO5Q8WYp7DLx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class IAMDataset(Dataset):\n",
        "    def __init__(self, root_dir, df, processor, max_target_length=128):\n",
        "        self.root_dir = root_dir\n",
        "        self.df = df\n",
        "        self.processor = processor\n",
        "        self.max_target_length = max_target_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # get file name + text \n",
        "        file_name = self.df['file_name'][idx]\n",
        "        text = self.df['text'][idx]\n",
        "        # prepare image (i.e. resize + normalize)\n",
        "        image = Image.open(self.root_dir + file_name).convert(\"RGB\")\n",
        "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
        "        # add labels (input_ids) by encoding the text\n",
        "        labels = self.processor.tokenizer(text, \n",
        "                                          padding=\"max_length\", \n",
        "                                          max_length=self.max_target_length).input_ids\n",
        "        # important: make sure that PAD tokens are ignored by the loss function\n",
        "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
        "\n",
        "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
        "        return encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzL7C60c-v-B"
      },
      "source": [
        "Let's initialize the training and evaluation datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KIa78c2W8uT9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrOCRProcessor\n",
        "\n",
        "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
        "train_dataset = IAMDataset(root_dir=r\"..\\\\data\\\\crops\\\\\",\n",
        "                           df=train_df,\n",
        "                           processor=processor)\n",
        "eval_dataset = IAMDataset(root_dir=r\"..\\\\data\\\\crops\\\\\",\n",
        "                           df=val_df,\n",
        "                           processor=processor)\n",
        "test_dataset = IAMDataset(root_dir=r\"..\\\\data\\\\crops\\\\\",\n",
        "                           df=test_df,\n",
        "                           processor=processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiwZLbMeLCfo",
        "outputId": "61ebe4b6-4bcd-411e-dcf3-ce669bf73246"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training examples: 93753\n",
            "Number of validation examples: 31251\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of training examples:\", len(train_dataset))\n",
        "print(\"Number of validation examples:\", len(eval_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p8JfQrx-6EM"
      },
      "source": [
        "Let's verify an example from the training dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwBNrfD78RA7",
        "outputId": "6dd081e0-d868-460a-9e81-2c48de7a09d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pixel_values torch.Size([3, 384, 384])\n",
            "labels torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "encoding = train_dataset[0]\n",
        "for k,v in encoding.items():\n",
        "  print(k, v.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN-3pf6T_uRe"
      },
      "source": [
        "We can also check the original image and decode the labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "QzgOFgD4_7Kw",
        "outputId": "b7aad103-2dbc-4205-bf16-d62a74023354"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=129x17>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image = Image.open(train_dataset.root_dir + train_df['file_name'][0]).convert(\"RGB\")\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMtfkDia-8tQ",
        "outputId": "0bc47221-61d4-4c54-9cdd-e389a16c7445"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ducati z1\n"
          ]
        }
      ],
      "source": [
        "labels = encoding['labels']\n",
        "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
        "label_str = processor.decode(labels, skip_special_tokens=True)\n",
        "print(label_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxU7TfoYBvg0"
      },
      "source": [
        "## Train a model\n",
        "\n",
        "Here, we initialize the TrOCR model from its pretrained weights. Note that the weights of the language modeling head are already initialized from pre-training, as the model was already trained to generate text during its pre-training stage. Refer to the paper for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRhvTRrGBIfy",
        "outputId": "de96977c-242a-4d2c-9bdf-7546b3349b74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-stage1 and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import VisionEncoderDecoderModel\n",
        "\n",
        "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-small-stage1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqNELu3cQix5"
      },
      "source": [
        "Importantly, we need to set a couple of attributes, namely:\n",
        "* the attributes required for creating the `decoder_input_ids` from the `labels` (the model will automatically create the `decoder_input_ids` by shifting the `labels` one position to the right and prepending the `decoder_start_token_id`, as well as replacing ids which are -100 by the pad_token_id)\n",
        "* the vocabulary size of the model (for the language modeling head on top of the decoder)\n",
        "* beam-search related parameters which are used when generating text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "sNNT1XS_CMgl"
      },
      "outputs": [],
      "source": [
        "# set special tokens used for creating the decoder_input_ids from the labels\n",
        "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
        "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
        "# make sure vocab size is set correctly\n",
        "model.config.vocab_size = model.config.decoder.vocab_size\n",
        "\n",
        "# set beam search parameters\n",
        "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
        "model.config.max_length = 64\n",
        "model.config.early_stopping = True\n",
        "model.config.no_repeat_ngram_size = 3\n",
        "model.config.length_penalty = 2.0\n",
        "model.config.num_beams = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt3G4Ts4-3RL"
      },
      "source": [
        "Next, we can define some training hyperparameters by instantiating the `training_args`. Note that there are many more parameters, all of which can be found in the [documentation](https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainingarguments). You can for example decide what the batch size is for training/evaluation, whether to use mixed precision training (lower memory), the frequency at which you want to save the model, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "o1G_rkDBzwid"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "use_fp16 = torch.cuda.is_available()\n",
        "\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    predict_with_generate=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    fp16=True,  # Set fp16 based on the availability of a CUDA device\n",
        "    output_dir=\"./df/\",\n",
        "    logging_steps=100,\n",
        "    save_steps=1000,\n",
        "    eval_steps=200,\n",
        "    num_train_epochs=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV6KY53xvOgC"
      },
      "source": [
        "We will evaluate the model on the Character Error Rate (CER), which is available in HuggingFace Datasets (see [here](https://huggingface.co/metrics/cer))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "yoXD3_P10DD4"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "cer_metric = load_metric(\"cer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G0R0sPFvfqT"
      },
      "source": [
        "The compute_metrics function takes an `EvalPrediction` (which is a NamedTuple) as input, and should return a dictionary. The model will return an EvalPrediction at evaluation, which consists of 2 things:\n",
        "* predictions: the predictions by the model.\n",
        "* label_ids: the actual ground-truth labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "Y36AcnvP0OZw"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
        "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"cer\": cer}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "use_fp16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vujH5mZ4-MXS"
      },
      "source": [
        "Let's train! We also provide the `default_data_collator` to the Trainer, which is used to batch together examples.\n",
        "\n",
        "Note that evaluation takes quite a long time, as we're using beam search for decoding, which requires several forward passes for a given example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "modelname='df/checkpoint-10000'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "id": "mcQMbxi10SDm",
        "outputId": "3750a131-4521-4d24-ccf6-b89622d0757b"
      },
      "source": [
        "from transformers import default_data_collator\n",
        "\n",
        "# instantiate trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    tokenizer=processor.feature_extractor,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=default_data_collator,\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(modelname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8tduj2fBGQK"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Note that after training, you can easily load the model using the .`from_pretrained(output_dir)` method.\n",
        "\n",
        "For inference on new images, I refer to my inference notebook, that can also be found in my [Transformers Tutorials repository](https://github.com/NielsRogge/Transformers-Tutorials) on Github."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "modelname='df/checkpoint-10000'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VisionEncoderDecoderModel(\n",
              "  (encoder): DeiTModel(\n",
              "    (embeddings): DeiTEmbeddings(\n",
              "      (patch_embeddings): DeiTPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): DeiTEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x DeiTLayer(\n",
              "          (attention): DeiTAttention(\n",
              "            (attention): DeiTSelfAttention(\n",
              "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
              "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
              "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): DeiTSelfOutput(\n",
              "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DeiTIntermediate(\n",
              "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): DeiTOutput(\n",
              "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
              "    (pooler): DeiTPooler(\n",
              "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (decoder): TrOCRForCausalLM(\n",
              "    (model): TrOCRDecoderWrapper(\n",
              "      (decoder): TrOCRDecoder(\n",
              "        (embed_tokens): Embedding(64044, 256, padding_idx=1)\n",
              "        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 256)\n",
              "        (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (layers): ModuleList(\n",
              "          (0-5): 6 x TrOCRDecoderLayer(\n",
              "            (self_attn): TrOCRAttention(\n",
              "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (activation_fn): ReLU()\n",
              "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): TrOCRAttention(\n",
              "              (k_proj): Linear(in_features=384, out_features=256, bias=True)\n",
              "              (v_proj): Linear(in_features=384, out_features=256, bias=True)\n",
              "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (output_projection): Linear(in_features=256, out_features=64044, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = VisionEncoderDecoderModel.from_pretrained(modelname)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch = next(iter(test_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['GALLARDO',\n",
              " 'Manzano.sonia@gmail.com',\n",
              " 'D',\n",
              " 'Gracia',\n",
              " '8060',\n",
              " '72953801X',\n",
              " 'Lituania',\n",
              " '1927']"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = batch[\"labels\"]\n",
        "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
        "label_str = processor.batch_decode(labels, skip_special_tokens=True)\n",
        "label_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "cer = load_metric(\"cer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running evaluation...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1cdfa506c30b48179344038f636666b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3907 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\bmazari.ieu2020\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1288: UserWarning: Using `max_length`'s default (64) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "print(\"Running evaluation...\")\n",
        "\n",
        "for batch in tqdm(test_dataloader):\n",
        "    # predict using generate\n",
        "    pixel_values = batch[\"pixel_values\"].to(device)\n",
        "    outputs = model.generate(pixel_values)\n",
        "\n",
        "    # decode\n",
        "    pred_str = processor.batch_decode(outputs, skip_special_tokens=True)\n",
        "    labels = batch[\"labels\"]\n",
        "    labels[labels == -100] = processor.tokenizer.pad_token_id\n",
        "    label_str = processor.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # add batch to metric\n",
        "    cer.add_batch(predictions=pred_str, references=label_str)\n",
        "\n",
        "final_score = cer.compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Character error rate on test set: 0.19241218071508168\n"
          ]
        }
      ],
      "source": [
        "print(\"Character error rate on test set:\", final_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load an image and preprocess it\n",
        "image = Image.open(r\"..\\\\data\\\\crops\\\\Fuente3parte_amistoso_7_18.jpg_5.jpg\").convert(\"RGB\")\n",
        "pixel_values = processor(image, return_tensors=\"pt\").pixel_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACQAAAAVCAIAAAChRNvHAAAH1ElEQVR4nC3U62/V9QHH8e/9dzu/3+9c2tOW9hx6tVTDJGGbGk1GkCc2QeYDxGY6spiFxCeEgc8ME58z5yVufeSEocZpN5C4EFgw23RLKAst1Erh7PS0pT3taU9/9/vv+9sD9he88s4n+cCLX34tSdLs7MzY2BiAPIoChFCWpZ7vSpKQpmmrtQ5RJstyFAUYU8ihJCqUYc45QkgUGWMMAK7kZMYYhBmEUJZFTdOyLPP9mBG9q6vLtp1CIUc8z3McZ2ZmdmRkRJZl27YB4KqqiFzyPI/zpFgsUkot2/C8IJcjGc+iKEpS8BBDCEmSJMsKQohSKggChBAAHkWJruvlTh1kLE2A4/wfC0qlwv3791dWVrq7u8vlMkLAsgyMcblcNk2z1VpHCDDGREF2bI8imjEoiqIoiAgDCGGScM5BLpeDMCOYCoIAAEiSxDLdwM8okWdu3d637ykAACmVSpqm/eubf7qu22g0MEYQZVsbzerAwNGjrzz99NOyLG9ubvi+z5iAEMEAAgCyLIMQIog551GY+DhMU1MURQgpxoBSKkkSIQQjgWDh6tW/7dv3VHvLJZZlMcaGRna9+eabSZKkaVIoFJhAGGNxHC4uLmIMZVkFABFCkiRSZSUIgihKfD+klAoChRBnHNuWj5EQ4tR1DACgqqq6rkMKV9c34jhdWFh+5JEKKpVKpmlub2+HYcwYKxZLhNCV5dX57+76fqgoqqrqnAPX8RDEoignCadUUBRFkiRZlhVFo5QlSSpLasZRHHGCBTVXkEQ1Cvn2tt2/s+fcR+cHByuXLl1FpmH7XjgwMFAsFj0vWFpamp6enpqaajabaZoSQhgTJUlRVV2Wc74XuH4AEEaEBlFs2o4XRGkGEw4AJmGSphmmghIlWXNjy3YDLV9oNFoIoWefHb9z5ztCCFl5sNRoNK5du3blyhXbtl944dDLL7+Spoks5TacJud8eXl5bm7uscceGxkZchwHIWQYRr3eCIKgUtnZ39+fUzQIMCUMY9JorFy/fn1+fv6He3985KXDmpLTNO2P5y8oORkZZvvmzZu2bd++PTMxceTtt88ePHgQY9zX1zc7O9vdtSOJef2/jUajIUmSZVlBFNquRwXxt++8V19cuvDxp6d/fSaMY8JYGCUA4utf/3329tw7775/5eq1JOa+7+/du1fX9VxOIIWCvmfPDwyjffToz0VR8n3Pdd1qtVKr1Q4cODA9Pd3V1XX+/PkzZ84kSYIxZRBjjH3f7+2tHDr0QrlcBgCeO3fuyIsT1Wr1xImTu3bt+vDDjzY2NkZHR03bylJp9+7da83Vzs5O5HluT093EPgpj9M06e3rkSTR850g9L759h99lR1Ly4uHfnrwwsfn6/W6YRi27TqO19FR3rmzP0lSSlmp2LnvJ/s/+eSTIAju3bv35JNP+r67sPD90tLi8PBwtVolhOi6LssC0fScH9ie71iWMbc8d+PGjVqtNjY29vzzz+dyuTAMBwcHdV3v7u6enp6+ePHi5pZx9uzZ+/fqxWKxr6/Ptm3bctfW1iRJKhQKn332qWVZhYJ28uSJt956a2OjmVOkP3z04UsTh6MoJlEUcJ6Y5vYXX/xp794fvfrqLzRNC4JAlmXOuSgKd+7c2bNnD+fJ44/vXl1tvvf+71pb2wMDQzcnf69N/WVtbW1ubu6JJ544ceL46vqqIAjf3/t+fuFu/1D/8OhwdWdva32ju7uMEIIwg3++9NcwDGu1WqVSURSl2WwqitLV1ZNlGedcFEXDMJaWli5cuGAYxvDI6IuHf1bo6Gw0GlNTU88999ytW/8ZHx8fHBzEGJqm+eijjx4+fHhoaOjkqV9Vq1XXMsudHUv1+o4dO1RNgJe+uvJwcN/3GWMPsyzTFgSBUsoYy7KsWCy2223Ogayoi/UHvZUBxtidudlSqcQ537XrEQBAlqWKomCCgiDo6+t78GBZ13UMM4Lwjq5Cc31LkkQkMuS6piIK5XJHEgaG0d7aWFdysiQwURJC30vS2DbNXC4nCRQjNDY2ttVaP/bLVzGAO/t6hwercegZ7ZbRbsEs4UkgMhT6ZqW3S1clUaSqIq08aObzaprGhGLU1VlKotSwDIGROE00VcmSOAaxLItYkShjoR8FnttZ6mhtbiOO+3p7HqwslopqFDkwzqIo6O7piqIA4zBJkihOPRRjlEIIBYopEyjhprGZz+cJocgwtjGiupqLoihNU4RIEAQQQtNo+37Y2dkpK2Icx5xzRRYlSeGc51Wto5gvaGoQuoBDghIs4CTyAACyKDAKfdeEEFJNs8ytQjEvMMFxHRKGYZqmlAiyLIdhGMcxACkAwHVdSmmWZWEYQgh930eQqKpmWZaqqqIoep6Xz2u6rkuS4LouxlhRJEEQfN/PsowxxhhzXZcSYlkWpRRCiGRZLpVKq6urCwsLWZZJkrS8vFyr1TjnD/larba9vf3wq+7evYsx9jzPsqzNzU3HccIwTJJkc3Oz3W7bth1F0dbWVqvVsm2bc27b9vz8vOM4aZq2Wi3SbrfDMJycnLRt+9ixY5VK5fLly/V6fWJiIp/Pf/vtv2dmZsbHx/fv33/jxvSXX15+5pkDr732miiKb7zxxsjI0Dvv/gZj/Prrr0uS9MEH72OMjx8/jhCanJwkhJw6dcr3vNOnT4+Ojn7++ef/AzZwOb3+ugAuAAAAAElFTkSuQmCC",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=36x21>"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[    0,     0,   717, 27840,     2]], device='cuda:0')"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "pixel_values = pixel_values.to('cuda')\n",
        "# generate the predicted text\n",
        "output = model.generate(pixel_values)\n",
        "output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Espot']"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# decode the output into a list of strings\n",
        "predicted_text = processor.batch_decode(output, skip_special_tokens=True)\n",
        "predicted_text"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Fine-tune TrOCR on IAM Handwriting Database using Seq2SeqTrainer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "99efefcdf8af6e3a1833e55397a5d357777bbc44dee36cecd6275d38f49d4482"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
